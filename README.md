
# Awesome-Remote-Sensing-Multimodal-Large-Language-Models


üî•üî•üî• **Multimodal Large Language Models for Remote Sensing: A Survey**  
**[Project Page][This Page](https://github.com/ZhanYang-nwpu/RS-MLLMs)** | 

**School of Artificial Intelligence, OPtics, and ElectroNics (iOPEN), Northwestern Polytechnical University**

<div align='center'> :sparkles: The <b>first survey</b> for Multimodal Large Language Models for Remote Sensing (RS-MLLMs). </div>  


‚ú®‚ú®‚ú® Behold our meticulously curated trove of RS-MLLMs resources!!!

üéâüöÄüí° The website will be updated in real-time to track the latest state of RS-MLLMs!!!

üìëüìöüîç Feast your eyes on an assortment of model architecture, training pipelines, datasets, comprehensive evaluation benchmarks, intelligent agents for remote sensing, techniques for instruction tuning, and much more. 

üåüüî•üì¢ A collection of remote sensing multimodal large language model papers focusing on the vision-language domain.


<p align="center">
    <img src="./images/1-timeline.jpg" width="100%" height="100%">
</p>

<font size=7><div align='center' > :apple: Multimodal Large Language Models for Remote Sensing </div></font>  

<p align="center">
    <img src="./images/6-timeline-agent.jpg" width="70%" height="100%">
</p>
<font size=7><div align='center' > :apple: Intelligent Agents for Remote Sensing </div></font>  



## Please share a <font color='orange'>STAR ‚≠ê</font> if this project does help




## üì¢ Latest Updates
In this repository, we will collect and document researchers and their outstanding work related to remote sensing multimodal large language model (vision-language).
- **The list will be continuously updated** üî•üî•
- üì¶ coming soon! üöÄ
- **May-22-2024**: The manuscript has been submitted for review. üî•üî•
  


---
<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Papers](#awesome-papers)
  - [Multimodal Large Language Models for Remote Sensing](#multimodal-large-language-models-for-remote-sensing)
  - [Intelligent Agents for Remote Sensing](#intelligent-agents-for-remote-sensing)
  - [Vision-Language Pre-training Models for Remote Sensing](#vision-language-pre-training-models-for-remote-sensing)
  - [Survey Papers for Remote Sensing Vision-Language Tasks](#survey-papers-for-remote-sensing-vision-language-tasks)
  - [Others](#others)
- [Awesome Datasets](#awesome-datasets)
  - [Datasets of Pre-Training for Alignment](#datasets-of-pre-training-for-alignment)
  - [Datasets of Multimodal Instruction Tuning](#datasets-of-multimodal-instruction-tuning)
- [Latest Evaluation Benchmarks for Remote Sensing Vision-Language Tasks](#latest-evaluation-benchmarks-for-remote-sensing-vision-language-tasks)
  - [Remote Sensing Image Captioning and Aerial Video Captioning](#remote-sensing-image-captioning-and-aerial-video-captioning)
  - [Remote Sensing Visual Question Answering and Remote Sensing Visual Grounding](#remote-sensing-visual-question-answering-and-remote-sensing-visual-grounding)
  - [Remote Sensing Image-Text Retrieval](#remote-sensing-image-text-retrieval)
  - [Remote Sensing Scene Classification](#remote-sensing-scene-classification)
---

# Awesome Papers

## Multimodal Large Language Models for Remote Sensing
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/BigData-KSU/RS-LLaVA.svg?style=social&label=Star) <br> [**RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery**](https://www.mdpi.com/2072-4292/16/9/1477) <br>Y. Bazi, L. Bashmal, M. M. Al Rahhal, R. Ricci, and F. Melgani. <br>| Remote Sensing | 2024-04-23 | [Github](https://github.com/BigData-KSU/RS-LLaVA) | - |
| ![Star](https://img.shields.io/github/stars/opendatalab/H2RSVLM.svg?style=social&label=Star) <br> [**H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model**](https://arxiv.org/abs/2403.20213.pdf) <br> C. Pang, W. Jiang, L. Jiayu, L. Yi, S. Jiaxing, L. Weijia, W. Xingxing, W. Shuai, F. Litong, X. Guisong, H.Conghui. <br>| arXiv | 2024-03-29 | [Github](https://github.com/opendatalab/H2RSVLM) | - |
| [**Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery**](https://arxiv.org/abs/2403.03790.pdf) <br>W. Zhang, M. Cai, T. Zhang, G. Lei, Y. Zhuang, and X. Mao.<br>| arXiv | 2024-03-06 | - | - |
| [**Large Language Models for Captioning and Retrieving Remote Sensing Images**](https://arxiv.org/abs/2402.06475.pdf) <br>J. D. Silva, J. Magalhaes, and D. Tuia.<br>| arXiv | 2024-02-09 | - | - |
| ![Star](https://img.shields.io/github/stars/NJU-LHRS/LHRS-Bot.svg?style=social&label=Star) <br> [**LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model**](https://arxiv.org/abs/2402.02544v2.pdf) <br>D. Muhtar, Z. Li, F. Gu, X. Zhang, and P. Xiao. <br>| arXiv | 2024-02-04 | [Github](https://github.com/NJU-LHRS/LHRS-Bot) | - |
| ![Star](https://img.shields.io/github/stars/wivizhang/EarthGPT.svg?style=social&label=Star) <br> [**EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain**](https://arxiv.org/abs/2401.16822.pdf) <br>W. Zhang, M. Cai, T. Zhang, Y. Zhuang, and X. Mao. <br>| arXiv | 2024-01-30 | [Github](https://github.com/wivizhang/EarthGPT) | - |
| ![Star](https://img.shields.io/github/stars/ZhanYang-nwpu/SkyEyeGPT.svg?style=social&label=Star) <br> [**SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**](https://arxiv.org/abs/2401.09712.pdf) <br>Y. Zhan, Z. Xiong, and Y. Yuan. <br>| arXiv | 2024-01-18 | [Github](https://github.com/ZhanYang-nwpu/SkyEyeGPT) | [Dataset](https://huggingface.co/datasets/ZhanYang-nwpu/SkyEye-968k) |
| ![Star](https://img.shields.io/github/stars/mbzuai-oryx/geochat.svg?style=social&label=Star) <br> [**GeoChat: Grounded Large Vision-Language Model for Remote Sensing**](http://arxiv.org/abs/2311.15826.pdf) <br>K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan. <br>| arXiv | 2023-11-24 | [Github](https://github.com/mbzuai-oryx/geochat) | accepted by CVPR-24 |
| ![Star](https://img.shields.io/github/stars/Lavender105/RSGPT.svg?style=social&label=Star) <br> [**RSGPT: A Remote Sensing Vision Language Model and Benchmark**](https://arxiv.org/abs/2307.15266.pdf) <br>Y. Hu, J. Yuan, and C. Wen. <br>| arXiv | 2023-07-28 | [Github](https://github.com/Lavender105/RSGPT) | - |


## Intelligent Agents for Remote Sensing
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots**](https://arxiv.org/abs/2404.15500.pdf) <br>S. Singh, M. Fore, D. Stamoulis, and D. Group.<br>| arXiv | 2024-04-23 | - | - |
| [**Evaluating Tool-Augmented Agents in Remote Sensing Platforms**](https://arxiv.org/abs/2405.00709v1.pdf) <br>S. Singh, M. Fore, and D. Stamoulis.<br>| arXiv | 2024-04-23 | - | - |
| ![Star](https://img.shields.io/github/stars/Chen-Yang-Liu/Change-Agent.svg?style=social&label=Star) <br> [**Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis**](https://arxiv.org/abs/2403.19646.pdf) <br>C. Liu, K. Chen, H. Zhang, Z. Qi, Z. Zou, and Z. Shi.<br>| arXiv | 2024-04-01 | [Github](https://github.com/Chen-Yang-Liu/Change-Agent) | - |
| ![Star](https://img.shields.io/github/stars/HaonanGuo/Remote-Sensing-ChatGPT.svg?style=social&label=Star) <br> [**Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models**](https://arxiv.org/abs/2401.09083v1.pdf) <br>H. Guo, X. Su, C. Wu, B. Du, L. Zhang, and D. Li.<br>| arXiv | 2024-01-17 | [Github](https://github.com/HaonanGuo/Remote-Sensing-ChatGPT) | - |
| [**Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis**](http://arxiv.org/abs/2310.04698.pdf) <br>S. Du, S. Tang, W. Wang, X. Li, and R. Guo.<br>| arXiv | 2023-10-07 | - | - |




## Vision-Language Pre-training Models for Remote Sensing
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/om-ai-lab/RS5M.svg?style=social&label=Star) <br> [**RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing**](https://arxiv.org/abs/2306.11300.pdf) <br>Z. Zhang, T. Zhao, Y. Guo, and J. Yin.<br>| arXiv | 2024-01-02 | [Github](https://github.com/om-ai-lab/RS5M) | - |
| ![Star](https://img.shields.io/github/stars/ChenDelong1999/RemoteCLIP.svg?style=social&label=Star) <br> [**RemoteCLIP: A Vision Language Foundation Model for Remote Sensing**](https://ieeexplore.ieee.org/abstract/document/10504785) <br>F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, and J. Zhou.<br>| T-GRS | 2024-04-18 | [Github](https://github.com/ChenDelong1999/RemoteCLIP) | [arXiv](https://arxiv.org/abs/2306.11029.pdf) |
| [**Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment**](https://openreview.net/pdf?id=w9tc699w3Z) <br>U. Mall, C. P. Phoo, M. K. Liu, C. Vondrick, B. Hariharan, and K. Bala.<br>| ICLR | 2024-01-16 | [Project](https://graft.cs.cornell.edu/) | [arXiv](http://arxiv.org/abs/2312.06960.pdf) |
 ![Star](https://img.shields.io/github/stars/ZhanYang-nwpu/PE-RSITR.svg?style=social&label=Star) <br> [**Parameter-Efficient Transfer Learning for Remote Sensing Image‚ÄìText Retrieval**](https://ieeexplore.ieee.org/abstract/document/10231134) <br>Y. Yuan, Y. Zhan, and Z. Xiong.<br>| T-GRS | 2023-08-28 | [Github](https://github.com/ZhanYang-nwpu/PE-RSITR) | [arXiv](https://arxiv.org/abs/2308.12509.pdf) |



## Survey Papers for Remote Sensing Vision-Language Tasks
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Vision-Language Models in Remote Sensing: Current progress and future trends**](https://ieeexplore.ieee.org/document/10506064) <br>X. Li, C. Wen, Y. Hu, Z. Yuan, and X. X. Zhu.<br>| MGRS | 2024-04-22 | - | - |
| [**Language Integration in Remote Sensing: Tasks, datasets, and future directions**](https://ieeexplore.ieee.org/abstract/document/10278197) <br>L. Bashmal, Y. Bazi, F. Melgani, M. M. Al Rahhal, and M. A. Al Zuair.<br>| MGRS | 2023-10-11 | - | - |
| [**Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey**](https://ieeexplore.ieee.org/abstract/document/10278197) <br>L. Jiao et al.<br>| JSTARS | 2023-09-18 | - | - |



## Others
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**On the Foundations of Earth and Climate Foundation Models**](https://arxiv.org/pdf/2405.04285.pdf) <br>X. X. Zhu et al.<br>| arXiv | 2024-05-07 | [Github](https://github.com/zhu-xlab/EarthFoundationModels) | - |
| [**On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications**](https://arxiv.org/abs/2312.17016.pdf) <br>C. Tan et al.<br>| arXiv | 2023-12-23 | - | - |
| ![Star](https://img.shields.io/github/stars/jonathan-roberts1/charting-new-territories.svg?style=social&label=Star) <br> [**Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs**](https://arxiv.org/abs/2311.14656.pdf) <br>J. Roberts, T. L√ºddecke, R. Sheikh, K. Han, and S. Albanie. <br>| arXiv | 2023-11-24 | [Github](https://github.com/jonathan-roberts1/charting-new-territories) | - |
| [**The Potential of Visual ChatGPT for Remote Sensing**](https://www.mdpi.com/2072-4292/15/13/3232) <br>L. P. Osco, E. L. de Lemos, W. N. Gon√ßalves, A. P. M. Ramos, and J. Marcato Junior.<br>| Remote Sensing | 2023-06-22 | - | - |




# Awesome Datasets

## Datasets of Pre-Training for Alignment
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing**](https://arxiv.org/abs/2402.11325.pdf) <br>Z. Yuan, Z. Xiong, L. Mou, and X. X. Zhu.<br>| arXiv | 2024-02-17 | - | - |
| ![Star](https://img.shields.io/github/stars/om-ai-lab/RS5M.svg?style=social&label=Star) <br> [**RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing**](https://arxiv.org/abs/2306.11300.pdf) <br>Z. Zhang, T. Zhao, Y. Guo, and J. Yin.<br>| arXiv | 2024-01-02 | [Github](https://github.com/om-ai-lab/RS5M) | - |
| ![Star](https://img.shields.io/github/stars/wangzhecheng/SkyScript.svg?style=social&label=Star) <br> [**SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing**](https://doi.org/10.1609/aaai.v38i6.28393) <br>Z. Wang, R. Prabha, T. Huang, J. Wu, and R. Rajagopal.<br>| AAAI | 2024-03-24 | [Github](https://github.com/wangzhecheng/SkyScript) | [arXiv](http://arxiv.org/abs/2312.12856.pdf) |


<p align="center">
    <img src="./images/itpair.jpg" width="80%" height="100%">
</p>


## Datasets of Multimodal Instruction Tuning
| Name | Paper | Link | Note |
|:-----|:-----:|:----:|:-----:|
| **RS-instructions** | [RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery](https://www.mdpi.com/2072-4292/16/9/1477) | [Link](https://github.com/BigData-KSU/RS-LLaVA) | 7,058 | 
| **SkyEye-968k** | [SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model](https://arxiv.org/abs/2401.09712.pdf) | [Link](https://huggingface.co/datasets/ZhanYang-nwpu/SkyEye-968k) | 968k | 
| **Multi-task Instruction** | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544v2.pdf) | [Link](https://github.com/NJU-LHRS/LHRS-Bot) | 42,322 |
| **MMRS-1M** | [EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain](https://arxiv.org/abs/2401.16822.pdf) | [Link](https://github.com/wivizhang/EarthGPT) | >1M |
| **RS-ClsQaGrd-Instruct** | [H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model](https://arxiv.org/abs/2403.20213.pdf) | [Link](https://github.com/opendatalab/H2RSVLM) | 78k |
| **MMShip** | [Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery](https://arxiv.org/abs/2403.03790.pdf) | [Link]() | 81k |
| **RS-Specialized-Instruct** |[H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model](https://arxiv.org/abs/2403.20213.pdf) | [Link](https://github.com/opendatalab/H2RSVLM) | 29.8k |
| **RS multimodal instruction** | [GeoChat: Grounded Large Vision-Language Model for Remote Sensing](http://arxiv.org/abs/2311.15826.pdf) | [Link](https://github.com/mbzuai-oryx/geochat) | 318k |
| **LHRS-Instruct** | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544v2.pdf) | [Link](https://github.com/NJU-LHRS/LHRS-Bot) | 39.8k |
| **HqDC-Instruct** | [H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model](https://arxiv.org/abs/2403.20213.pdf) | [Link](https://github.com/opendatalab/H2RSVLM) | 30k |



<p align="center">
    <img src="./images/instruct.jpg" width="80%" height="100%">
</p>




# Latest Evaluation Benchmarks for Remote Sensing Vision-Language Tasks

## Remote Sensing Image Captioning and Aerial Video Captioning
<p align="center">
    <img src="./images/caption.jpg" width="80%" height="100%">
</p>


## Remote Sensing Visual Question Answering and Remote Sensing Visual Grounding
<p align="center">
    <img src="./images/vqavg.jpg" width="80%" height="100%">
</p>



## Remote Sensing Image-Text Retrieval
<p align="center">
    <img src="./images/itretrieval.jpg" width="80%" height="100%">
</p>

## Remote Sensing Scene Classification
<p align="center">
    <img src="./images/rssc.jpg" width="80%" height="100%">
</p>



## ü§ñ Contact
If you have any questions about this project, please feel free to contact zhanyangnwpu@gmail.com.



