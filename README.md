# Awesome-Remote-Sensing-Multimodal-Large-Language-Model (Vision-Language)


üì¢ A collection of remote sensing multimodal large language model papers focusing on the vision-language domain.

##### Author: Yang Zhan
**School of Artificial Intelligence, OPtics, and ElectroNics (iOPEN), Northwestern Polytechnical University**
## Please share a <font color='orange'>STAR ‚≠ê</font> if this project does help


## üì¢ Latest Updates
In this repository, we will collect and document researchers and their outstanding work related to remote sensing multimodal large language model (vision-language).
- **The list will be continuously updated** üî•üî•
- üì¶ coming soon! üöÄ
---



## Content
- [Papers](#papers)
- [Remote Sensing Vision-Language Dataset](#Remote-Sensing-Vision-Language-Dataset)
- [related: Remote Sensing Vision-Language Foundation Models](#related)


## Papers
- üî• **Jan-30-24: EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain**

arXiv 2024 (arXiv:2401.16822). *W. Zhang, M. Cai, T. Zhang, Y. Zhuang, and X. Mao.* [[Paper](https://arxiv.org/abs/2401.16822)][[Code]()]


- üî• **Jan-18-24: SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**

arXiv 2024 (arXiv:2401.09712). *Y. Zhan, Z. Xiong, and Y. Yuan.* [[Paper](https://arxiv.org/abs/2401.09712)][[Code](https://github.com/ZhanYang-nwpu/SkyEyeGPT)]

- üî• **Nov-30-23: Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs**

arXiv 2023 (arXiv:2311.14656). *J. Roberts, T. L√ºddecke, R. Sheikh, K. Han, and S. Albanie.* [[Paper](https://arxiv.org/abs/2311.14656)][[Code](https://github.com/jonathan-roberts1/charting-new-territories)]

- üî• **Nov-28-23: GeoChat: Grounded Large Vision-Language Model for Remote Sensing**

arXiv 2023 (arXiv:2311.15826). *K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan.* [[Paper](http://arxiv.org/abs/2311.15826)][[Code](https://github.com/mbzuai-oryx/geochat)]

- üî• **Jul-28-23: RSGPT: A Remote Sensing Vision Language Model and Benchmark** 

arXiv 2023 (arXiv:2307.15266). *Y. Hu, J. Yuan, and C. Wen.* [[Paper](https://arxiv.org/abs/2307.15266)][[Code](https://github.com/Lavender105/RSGPT)]

## Remote Sensing Vision-Language Dataset
- üî• **Jan-2-24: RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing**

arXiv 2023 (arXiv:2306.11300). *Z. Zhang, T. Zhao, Y. Guo, and J. Yin.* [[Paper](https://arxiv.org/abs/2306.11300)][[Code](https://github.com/om-ai-lab/RS5M)]

- üî• **Dec-20-23: SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing**

AAAI 2024 (arXiv:2312.12856). *Z. Wang, R. Prabha, T. Huang, J. Wu, and R. Rajagopal.* [[Paper](http://arxiv.org/abs/2312.12856)][[Code](https://github.com/wangzhecheng/SkyScript)]


## related: Remote Sensing Vision-Language Foundation Models
- üî• **Jan-2-24: RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing**

arXiv 2023 (arXiv:2306.11300). *Z. Zhang, T. Zhao, Y. Guo, and J. Yin.* [[Paper](https://arxiv.org/abs/2306.11300)][[Code](https://github.com/om-ai-lab/RS5M)]

- üî• **Dec-12-23: Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment**

arXiv 2023 (arXiv:2312.06960). *U. Mall, C. P. Phoo, M. K. Liu, C. Vondrick, B. Hariharan, and K. Bala.* [[Paper](http://arxiv.org/abs/2312.06960)][[Code]:Null]

- üî• **Aug-10-23: RemoteCLIP: A Vision Language Foundation Model for Remote Sensing**

arXiv 2023 (arXiv:2306.11029). *F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, and J. Zhou.* [[Paper](https://arxiv.org/abs/2306.11029)][[Code](https://github.com/ChenDelong1999/RemoteCLIP)]



