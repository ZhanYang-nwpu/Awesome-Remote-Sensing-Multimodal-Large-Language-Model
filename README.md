
# Awesome-Remote-Sensing-Multimodal-Large-Language-Models


üî•üî•üî• **Multimodal Large Language Models for Remote Sensing: A Survey**  
**[Project Page][This Page](https://github.com/ZhanYang-nwpu/RS-MLLMs)** | 

**School of Artificial Intelligence, OPtics, and ElectroNics (iOPEN), Northwestern Polytechnical University**

<div align='center'> :sparkles: The <b>first survey</b> for Multimodal Large Language Models for Remote Sensing (RS-MLLMs). </div>  


‚ú®‚ú®‚ú® Behold our meticulously curated trove of RS-MLLMs resources!!!

üéâüöÄüí° The website will be updated in real-time to track the latest state of RS-MLLMs!!!

üìëüìöüîç Feast your eyes on an assortment of model architecture, training pipelines, datasets, comprehensive evaluation benchmarks, intelligent agents for remote sensing, techniques for instruction tuning, and much more. 

üåüüî•üì¢ A collection of remote sensing multimodal large language model papers focusing on the vision-language domain.


<p align="center">
    <img src="./images/1-timeline.jpg" width="100%" height="100%">
</p>

<font size=7><div align='center' > :apple: Multimodal Large Language Models for Remote Sensing </div></font>  

<p align="center">
    <img src="./images/6-timeline-agent.jpg" width="70%" height="100%">
</p>
<font size=7><div align='center' > :apple: Intelligent Agents for Remote Sensing </div></font>  



## Please share a <font color='orange'>STAR ‚≠ê</font> if this project does help




## üì¢ Latest Updates
In this repository, we will collect and document researchers and their outstanding work related to remote sensing multimodal large language model (vision-language).
- **The list will be continuously updated** üî•üî•
- üì¶ coming soon! üöÄ
- **May-22-2024**: The **first RS-MLLMs review** manuscript has been submitted for review. üî•üî•
  


---
<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Papers](#awesome-papers)
  - [Multimodal Large Language Models for Remote Sensing](#multimodal-large-language-models-for-remote-sensing)
  - [Intelligent Agents for Remote Sensing](#intelligent-agents-for-remote-sensing)
  - [Vision-Language Pre-training Models for Remote Sensing](#vision-language-pre-training-models-for-remote-sensing)
  - [Survey Papers for Remote Sensing Vision-Language Tasks](#survey-papers-for-remote-sensing-vision-language-tasks)
  - [Others](#others)
- [Awesome Datasets](#awesome-datasets)
  - [Datasets of Pre-Training for Alignment](#datasets-of-pre-training-for-alignment)
  - [Datasets of Multimodal Instruction Tuning](#datasets-of-multimodal-instruction-tuning)
- [Latest Evaluation Benchmarks for Remote Sensing Vision-Language Tasks](#latest-evaluation-benchmarks-for-remote-sensing-vision-language-tasks)
  - [Remote Sensing Image Captioning and Aerial Video Captioning](#remote-sensing-image-captioning-and-aerial-video-captioning)
  - [Remote Sensing Visual Question Answering and Remote Sensing Visual Grounding](#remote-sensing-visual-question-answering-and-remote-sensing-visual-grounding)
  - [Remote Sensing Image-Text Retrieval](#remote-sensing-image-text-retrieval)
  - [Remote Sensing Scene Classification](#remote-sensing-scene-classification)
---

# Awesome Papers

## Multimodal Large Language Models for Remote Sensing
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|  [**A semantic-enhanced multi-modal remote sensing foundation model for Earth observation**](https://www.nature.com/articles/s42256-025-01078-8) <br>Wu, K., Zhang, Y., Ru, L., Dang, B., Lao, J., Yu, L., ... & Li, Y. <br>| Nature Machine Intelligence | 2025-08-04 | - | - |
|  [**Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling**](https://arxiv.org/pdf/2506.21863) <br>Sungjune Park, Yeongyun Kim, Se Yeon Kim, Yong Man Ro <br>| arXiv | 2025-06-27 | - | - |
|  ![Star](https://img.shields.io/github/stars/shuyansy/EarthMind.svg?style=social&label=Star) <br> [**EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation with Large Multimodal Models**](https://arxiv.org/abs/2506.01667) <br>Yan Shu, Bin Ren, Zhitong Xiong, Danda Pani Paudel, Luc Van Gool, Begum Demir, Nicu Sebe, Paolo Rota <br>| arXiv | 2025-06-02 | [Github](https://github.com/shuyansy/EarthMind) | - |
|  ![Star](https://img.shields.io/github/stars/MiliLab/GeoLLaVA-8K.svg?style=social&label=Star) <br> [**GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution**](https://arxiv.org/abs/2505.21375) <br>Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, Hongzhen Wang, Wenjing Yang, Bo Du, Jing Zhang <br>| arXiv | 2025-05-27 | [Github](https://github.com/MiliLab/GeoLLaVA-8K) | - |
|  [**TinyRS-R1: Compact Multimodal Language Model for Remote Sensing**](https://arxiv.org/abs/2505.12099) <br>Aybora Koksal, A. Aydin Alatan <br>| arXiv | 2025-05-17 | - | - |
|  [**EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand Multi-Source Remote Sensing Imagery**](https://arxiv.org/abs/2504.12795) <br>Wei Zhang, Miaoxin Cai, Yaqian Ning, Tong Zhang, Yin Zhuang, He Chen, Jun Li, Xuerui Mao <br>| arXiv | 2025-04-17 | - | - |
| ![Star](https://img.shields.io/github/stars/earth-insights/SegEarth-R1.svg?style=social&label=Star) <br> [**SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model**](https://arxiv.org/abs/2504.09644) <br>Kaiyu Li, Zepeng Xin, Li Pang, Chao Pang, Yupeng Deng, Jing Yao, Guisong Xia, Deyu Meng, Zhi Wang, Xiangyong Cao <br>| arXiv | 2025-04-13 | [Github](https://github.com/earth-insights/SegEarth-R1)  | - |
| ![Star](https://img.shields.io/github/stars/hiyamdebary/EarthDial.svg?style=social&label=Star) <br> [**EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues**](https://openaccess.thecvf.com/content/CVPR2025/html/Soni_EarthDial_Turning_Multi-sensory_Earth_Observations_to_Interactive_Dialogues_CVPR_2025_paper.html) <br>Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan <br>| CVPR-25 | 2025-04-07 | [Github](https://github.com/hiyamdebary/EarthDial)  | - |
| ![Star](https://img.shields.io/github/stars/XiangTodayEatsWhat/EagleVision.svg?style=social&label=Star) <br> [**EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing**](https://arxiv.org/abs/2503.23330) <br>H. Jiang, J. Yin, Q. Wang, J. Feng, G. Chen <br>| arXiv | 2025-03-30 | [Github](https://github.com/XiangTodayEatsWhat/EagleVision)  | - |
|  [**OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence**](https://arxiv.org/abs/2503.16326) <br>Yuan, L., Mo, F., Huang, K., Wang, W., Zhai, W., Zhu, X., ... & Nie, J. Y. <br>| arXiv | 2025-03-20 | - | - |
| ![Star](https://img.shields.io/github/stars/TianHuiLab/Falcon.svg?style=social&label=Star) <br> [**Falcon: A Remote Sensing Vision-Language Foundation Model**](https://arxiv.org/abs/2503.11070) <br>K. Yao, N. Xu, R. Yang, Y. Xu, Z. Gao, T. Kitrungrotsakul, Y. Ren, P. Zhang, J. Wang, N. Wei, C. Li <br>| arXiv | 2025-03-14 | [Github](https://github.com/TianHuiLab/Falcon)  | - |
|  ![Star](https://img.shields.io/github/stars/VisionXLab/LRS-VQA.svg?style=social&label=Star) <br>[**When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning**](https://arxiv.org/abs/2503.07588) <br>J. Luo, Y. Zhang, X. Yang, K. Wu, Q. Zhu, L. Liang, J. Chen, Y. Li<br>| arXiv | 2025-03-10 | [Github](https://github.com/VisionXLab/LRS-VQA)  | - |
|  ![Star](https://img.shields.io/github/stars/demo1shining/Co-LLaVA.svg?style=social&label=Star) <br>[**Co-LLaVA: Efficient Remote Sensing Visual Question Answering via Model Collaboration**](https://www.mdpi.com/2072-4292/17/3/466) <be>Liu, F., Dai, W., Zhang, C., Zhu, J., Yao, L., & Li, X. <br>| arXiv | 2025-01-29 | [Github](https://github.com/demo1shining/Co-LLaVA)  | - |
|  ![Star](https://img.shields.io/github/stars/mbzuai-oryx/GeoPixel.svg?style=social&label=Star) <br>[**GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing**](https://arxiv.org/abs/2501.13925) <be>A. Shabbir, M. Zumri, M. Bennamoun, F. S. Khan, S. Khan <br>| arXiv | 2025-01-23 | [Github](https://github.com/mbzuai-oryx/GeoPixel)  | - |
|  ![Star](https://img.shields.io/github/stars/Norman-Ou/GeoPix.svg?style=social&label=Star) <br>[**GeoPix: Multi-Modal Large Language Model for Pixel-level Image Understanding in Remote Sensing**](https://arxiv.org/abs/2501.06828) <be>R. Ou, Y. Hu, F. Zhang, J. Chen, Y. Liu <br>| arXiv | 2025-01-12 | [Github](https://github.com/Norman-Ou/GeoPix)  | accepted by GRSM-25 |
|  ![Star](https://img.shields.io/github/stars/xuliu-cyber/RSUniVLM.svg?style=social&label=Star) <br>[**RSUniVLM: A Unified Vision Language Model for Remote Sensing via Granularity-oriented Mixture of Experts**](https://arxiv.org/abs/2412.05679) <be>Xu Liu, Zhouhui Lian <br>| arXiv | 2024-12-10 | [Github](https://github.com/xuliu-cyber/RSUniVLM)  | - |
|  [**RingMoGPT: A Unified Remote Sensing Foundation Model for Vision, Language, and grounded tasks**](https://ieeexplore.ieee.org/abstract/document/10777289) <br>P. Wang, H. Hu, B. Tong, Z. Zhang, F. Yao, Y. Feng, Z. Zhu, H. Chang, W. Diao, Q. Ye, and X. Sun <br>| T-GRS | 2024-12-04 | - | - |
| ![Star](https://img.shields.io/github/stars/zytx121/GeoGround.svg?style=social&label=Star) <br> [**GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding**](https://arxiv.org/abs/2411.11904) <br>Y. Zhou, M. Lan, X. Li, Y. Ke, X. Jiang, L. Feng, and W. Zhang <br>| arXiv | 2024-11-16 | [Github](https://github.com/zytx121/GeoGround) | - |
|  [**Large Vision-Language Models for Remote Sensing Visual Question Answering**](https://arxiv.org/abs/2411.10857) <br>S. Siripong, A. Chaiyapan, and T. Phonchai <br>| arXiv | 2024-11-16 | - | - |
| ![Star](https://img.shields.io/github/stars/NJU-LHRS/LHRS-Bot.svg?style=social&label=Star) <br> [**LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation**](https://arxiv.org/abs/2411.09301) <br>Z. Li, D. Muhtar, F. Gu, X. Zhang, P. Xiao, G. He, and X. Zhu <br>| arXiv | 2024-11-14 | [Github](https://github.com/NJU-LHRS/LHRS-Bot/tree/nova) | accepted by ISPRS-25 |
|  [**RS-MoE: Mixture of Experts for Remote Sensing Image Captioning and Visual Question Answering**](https://arxiv.org/abs/2411.01595) <br>Lin, H., Hong, D., Ge, S., Luo, C., Jiang, K., Jin, H., and Wen, C<br>| arXiv | 2024-11-03 | - | accepted by TGRS-25 |
| ![Star](https://img.shields.io/github/stars/HosamGen/GeoLLaVA.svg?style=social&label=Star) <br> [**GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing**](https://arxiv.org/pdf/2410.19552) <br>Elgendy, H., Sharshar, A., Aboeitta, A., Ashraf, Y., and Guizani, M. <br>| arXiv | 2024-10-25 | [Github](https://github.com/HosamGen/GeoLLaVA) | - |
| ![Star](https://img.shields.io/github/stars/ermongroup/TEOChat.svg?style=social&label=Star) <br> [**TEOChat: Large Language and Vision Assistant for Temporal Earth Observation Data**](https://arxiv.org/pdf/2410.06234) <br>J. Irvin, Jeremy Andrew, et al. <br>| arXiv | 2024-10-08 | [Github](https://github.com/ermongroup/TEOChat) | - |
|  [**CDChat: A Large Multimodal Model for Remote Sensing Change Description**](https://arxiv.org/abs/2409.16261) <br>Noman, M., Ahsan, N., Naseer, M., Cholakkal, H., Anwer, R. M., Khan, S., & Khan, F. S C<br>| arXiv | 2024-09-24 | - | - |
| ![Star](https://img.shields.io/github/stars/wivizhang/EarthMarker.svg?style=social&label=Star) <br> [**EarthMarker: A Visual Prompting MLLM for Region-level and Point-level Remote Sensing Imagery Comprehension**](https://arxiv.org/pdf/2407.13596) <br>Zhang, W., Cai, M., Zhang, T., Zhuang, Y., and Mao, X. <br>| arXiv | 2024-07-18 | [Github](https://github.com/wivizhang/EarthMarker) | accepted by TGRS-24 |
| ![Star](https://img.shields.io/github/stars/Luo-Z13/SkySenseGPT.svg?style=social&label=Star) <br> [**SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding**](http://arxiv.org/abs/2406.10100) <br>J. Luo et al. <br>| arXiv | 2024-06-14 | [Github](https://github.com/Luo-Z13/SkySenseGPT) | - |
| ![Star](https://img.shields.io/github/stars/BigData-KSU/RS-LLaVA.svg?style=social&label=Star) <br> [**RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery**](https://www.mdpi.com/2072-4292/16/9/1477) <br>Y. Bazi, L. Bashmal, M. M. Al Rahhal, R. Ricci, and F. Melgani. <br>| Remote Sensing | 2024-04-23 | [Github](https://github.com/BigData-KSU/RS-LLaVA) | - |
| ![Star](https://img.shields.io/github/stars/opendatalab/H2RSVLM.svg?style=social&label=Star) <br> [**VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis**](https://ojs.aaai.org/index.php/AAAI/article/view/32683) [H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model] <br> C. Pang, W. Jiang, L. Jiayu, L. Yi, S. Jiaxing, L. Weijia, W. Xingxing, W. Shuai, F. Litong, X. Guisong, H.Conghui. <br>| arXiv | 2024-03-29 | [Github](https://github.com/opendatalab/VHM)  | accepted by AAAI-25 |
| [**Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery**](https://arxiv.org/abs/2403.03790.pdf) <br>W. Zhang, M. Cai, T. Zhang, G. Lei, Y. Zhuang, and X. Mao.<br>| arXiv | 2024-03-06 | - | accepted by JSTARS-24 |
| [**Large Language Models for Captioning and Retrieving Remote Sensing Images**](https://arxiv.org/abs/2402.06475.pdf) <br>J. D. Silva, J. Magalhaes, and D. Tuia.<br>| arXiv | 2024-02-09 | - | - |
| ![Star](https://img.shields.io/github/stars/NJU-LHRS/LHRS-Bot.svg?style=social&label=Star) <br> [**LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model**](https://arxiv.org/abs/2402.02544v2.pdf) <br>D. Muhtar, Z. Li, F. Gu, X. Zhang, and P. Xiao. <br>| arXiv | 2024-02-04 | [Github](https://github.com/NJU-LHRS/LHRS-Bot) | accepted by ECCV-24 |
| ![Star](https://img.shields.io/github/stars/wivizhang/EarthGPT.svg?style=social&label=Star) <br> [**EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain**](https://arxiv.org/abs/2401.16822.pdf) <br>W. Zhang, M. Cai, T. Zhang, Y. Zhuang, and X. Mao. <br>| arXiv | 2024-01-30 | [Github](https://github.com/wivizhang/EarthGPT) | accepted by IEEE-TGRS|
| ![Star](https://img.shields.io/github/stars/ZhanYang-nwpu/SkyEyeGPT.svg?style=social&label=Star) <br> [**SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**](https://arxiv.org/abs/2401.09712.pdf) <br>Y. Zhan, Z. Xiong, and Y. Yuan. <br>| arXiv | 2024-01-18 | [Github](https://github.com/ZhanYang-nwpu/SkyEyeGPT) | accepted by ISPRS-25 |
| ![Star](https://img.shields.io/github/stars/mbzuai-oryx/geochat.svg?style=social&label=Star) <br> [**GeoChat: Grounded Large Vision-Language Model for Remote Sensing**](http://arxiv.org/abs/2311.15826.pdf) <br>K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan. <br>| arXiv | 2023-11-24 | [Github](https://github.com/mbzuai-oryx/geochat) | accepted by CVPR-24 |
| ![Star](https://img.shields.io/github/stars/Lavender105/RSGPT.svg?style=social&label=Star) <br> [**RSGPT: A Remote Sensing Vision Language Model and Benchmark**](https://arxiv.org/abs/2307.15266.pdf) <br>Y. Hu, J. Yuan, and C. Wen. <br>| arXiv | 2023-07-28 | [Github](https://github.com/Lavender105/RSGPT) | accepted by ISPRS-25 |


## Intelligent Agents for Remote Sensing
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**RS-Agent: Automating Remote Sensing Tasks through Intelligent Agents**](https://arxiv.org/abs/2406.07089) <br>W. Xu, Z. Yu, Y. Wang, J. Wang, and M. Peng.<br>| arXiv | 2024-06-11 | - | - |
| [**GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots**](https://arxiv.org/abs/2404.15500.pdf) <br>S. Singh, M. Fore, D. Stamoulis, and D. Group.<br>| arXiv | 2024-04-23 | - | - |
| [**Evaluating Tool-Augmented Agents in Remote Sensing Platforms**](https://arxiv.org/abs/2405.00709v1.pdf) <br>S. Singh, M. Fore, and D. Stamoulis.<br>| arXiv | 2024-04-23 | - | - |
| ![Star](https://img.shields.io/github/stars/Chen-Yang-Liu/Change-Agent.svg?style=social&label=Star) <br> [**Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis**](https://arxiv.org/abs/2403.19646.pdf) <br>C. Liu, K. Chen, H. Zhang, Z. Qi, Z. Zou, and Z. Shi.<br>| arXiv | 2024-04-01 | [Github](https://github.com/Chen-Yang-Liu/Change-Agent) | - |
| ![Star](https://img.shields.io/github/stars/HaonanGuo/Remote-Sensing-ChatGPT.svg?style=social&label=Star) <br> [**Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models**](https://arxiv.org/abs/2401.09083v1.pdf) <br>H. Guo, X. Su, C. Wu, B. Du, L. Zhang, and D. Li.<br>| arXiv | 2024-01-17 | [Github](https://github.com/HaonanGuo/Remote-Sensing-ChatGPT) | - |
| [**Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis**](http://arxiv.org/abs/2310.04698.pdf) <br>S. Du, S. Tang, W. Wang, X. Li, and R. Guo.<br>| arXiv | 2023-10-07 | - | - |




## Vision-Language Pre-training Models for Remote Sensing
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/MitsuiChen14/LRSCLIP.svg?style=social&label=Star) <br> [**LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text**](https://arxiv.org/abs/2503.19311) <br>W. Chen, J. Chen, Y. Deng, J. Chen, Y. Feng, Z. Xi, D. Liu, K. Li, Y. Meng.<br>| arXiv | 2025-03-25 | [Github](https://github.com/MitsuiChen14/LRSCLIP) | [arXiv](https://arxiv.org/abs/2503.19311.pdf) |
| ![Star](https://img.shields.io/github/stars/xiong-zhitong/DOFA-CLIP.svg?style=social&label=Star) <br> [**DOFA-CLIP: Multimodal Vision-Language Foundation Models for Earth Observation**](https://arxiv.org/abs/2503.06312) <br>Zhitong Xiong, Yi Wang, Weikang Yu, Adam J Stewart, Jie Zhao, Nils Lehmann, Thomas Dujardin, Zhenghang Yuan, Pedram Ghamisi, Xiao Xiang Zhu.<br>| arXiv | 2025-03-08 | [Github](https://github.com/xiong-zhitong/DOFA-CLIP) | - 
| ![Star](https://img.shields.io/github/stars/om-ai-lab/RS5M.svg?style=social&label=Star) <br> [**RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing**](https://arxiv.org/abs/2306.11300.pdf) <br>Z. Zhang, T. Zhao, Y. Guo, and J. Yin.<br>| arXiv | 2024-01-02 | [Github](https://github.com/om-ai-lab/RS5M) | accepted by IEEE-TGRS |
| ![Star](https://img.shields.io/github/stars/ChenDelong1999/RemoteCLIP.svg?style=social&label=Star) <br> [**RemoteCLIP: A Vision Language Foundation Model for Remote Sensing**](https://ieeexplore.ieee.org/abstract/document/10504785) <br>F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, and J. Zhou.<br>| T-GRS | 2024-04-18 | [Github](https://github.com/ChenDelong1999/RemoteCLIP) | [arXiv](https://arxiv.org/abs/2306.11029.pdf) |
| [**Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment**](https://openreview.net/pdf?id=w9tc699w3Z) <br>U. Mall, C. P. Phoo, M. K. Liu, C. Vondrick, B. Hariharan, and K. Bala.<br>| ICLR | 2024-01-16 | [Project](https://graft.cs.cornell.edu/) | [arXiv](http://arxiv.org/abs/2312.06960.pdf) |
 ![Star](https://img.shields.io/github/stars/lx709/RS-CLIP.svg?style=social&label=Star) <br> [**RS-CLIP: Zero Shot Remote Sensing Scene Classification via Contrastive Vision-Language Supervision**](https://www.sciencedirect.com/science/article/pii/S1569843223003217) <br>X. Li, C. Wen, Y. Hu, and N. Zhou.<br>| JAG | 2023-09-18 | [Github](https://github.com/lx709/RS-CLIP) | - |
 ![Star](https://img.shields.io/github/stars/ZhanYang-nwpu/PE-RSITR.svg?style=social&label=Star) <br> [**Parameter-Efficient Transfer Learning for Remote Sensing Image‚ÄìText Retrieval**](https://ieeexplore.ieee.org/abstract/document/10231134) <br>Y. Yuan, Y. Zhan, and Z. Xiong.<br>| T-GRS | 2023-08-28 | [Github](https://github.com/ZhanYang-nwpu/PE-RSITR) | [arXiv](https://arxiv.org/abs/2308.12509.pdf) |



## Survey Papers for Remote Sensing Vision-Language Tasks
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Vision-Language Modeling Meets Remote Sensing: Models, datasets, and perspectives**](https://ieeexplore.ieee.org/abstract/document/11028078/) <br>Xingxing Weng; Chao Pang; Gui-Song Xia.<br>| GRSM | 2025-06-09 | - | - |
| ![Star](https://img.shields.io/github/stars/IRIP-BUAA/A-Survey-on-Data-Synthesis-and-Augmentation-for-Large-Language-Models.svg?style=social&label=Star) <br>[**A Survey on Remote Sensing Foundation Models: From Vision to Multimodality**](https://arxiv.org/abs/2412.02573) <br>Z. Huang, H. Yan, Q. Zhan, S. Yang, M. Zhang, C. Zhang, Y. Lei, Z. Liu, Q. Liu, Y. Wang <br>| arXiv | 2025-03-28 | [Github](https://github.com/IRIP-BUAA/A-Survey-on-Data-Synthesis-and-Augmentation-for-Large-Language-Models) | [arXiv](https://arxiv.org/abs/2503.22081) |
| [**GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing**](https://arxiv.org/abs/2503.12490) <br>Z. Zhang, H. Shen, T. Zhao, B. Chen, Z. Guan, Y. Wang, X. Jia, Y. Cai, Y. Shang, J. Yin.<br>| arXiv | 2025-03-16 | - | - |
| [**When Remote Sensing Meets Foundation Model: A Survey and Beyond**](https://www.mdpi.com/2072-4292/17/2/179) <br>Huo, Chunlei; Chen, Keming; Zhang, Shuaihao; Wang, Zeyu; Yan, Heyu; Shen, Jing; Hong, Yuyang; Qi, Geqi; Fang, Hongmei; Wang, Zihan.<br>| remote sensing | 2025-01-07 | - | - |
| ![Star](https://img.shields.io/github/stars/Chen-Yang-Liu/Awesome-RS-Temporal-VLM.svg?style=social&label=Star) <br>[**Remote Sensing Temporal Vision-Language Models: A Comprehensive Survey**](https://arxiv.org/abs/2412.02573) <br>C. Liu, J. Zhang, K. Chen, M. Wang, Z. Zou, and Z. Shi <br>| arXiv | 2024-12-03 | [Github](https://github.com/Chen-Yang-Liu/Awesome-RS-Temporal-VLM) | [arXiv](https://arxiv.org/abs/2412.02573) |
| [**From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing**](https://arxiv.org/abs/2411.05826) <br>X. Sun, B. Peng, C. Zhang, F. Jin, Q. Niu, J. Liu, K. Chen, M. Li, P. Feng, Z. Bi, M. Liu, and Y. Zhang.<br>| arXiv | 2024-11-05 | - | - |
| ![Star](https://img.shields.io/github/stars/xiaoaoran/awesome-RSFMs.svg?style=social&label=Star) <br>[**Foundation Models for Remote Sensing and Earth Observation: A Survey**](https://arxiv.org/abs/2410.16602) <br>A. Xiao, W. Xuan, J. Wang, J. Huang, D. Tao, S. Lu, and N. Yokoya.<br>| arXiv | 2024-10-22 | [Github](https://github.com/xiaoaoran/awesome-RSFMs) | accepted by GRSM-25 |
| ![Star](https://img.shields.io/github/stars/taolijie11111/VLMs-in-RS-review.svg?style=social&label=Star) <br>[**Advancements in Visual Language Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques**](https://arxiv.org/abs/2410.17283) <br>L. Tao, H. Zhang, H. Jing, Y. Liu, K. Yao, C. Li, and X. Xue.<br>| arXiv | 2024-10-15 | [Github](https://github.com/taolijie11111/VLMs-in-RS-review) | accepted by remote sensing-25 |
| ![Star](https://img.shields.io/github/stars/zytx121/Awesome-VLGFM.svg?style=social&label=Star) <br>[**Towards Vision-Language Geo-Foundation Model: A Survey**](http://arxiv.org/abs/2406.09385) <br>Y. Zhou, L. Feng, Y. Ke, X. Jiang, J. Yan, and W. Zhang.<br>| arXiv | 2024-06-13 | [Github](https://github.com/zytx121/Awesome-VLGFM) | [arXiv](http://arxiv.org/abs/2406.09385) |
| [**Vision-Language Models in Remote Sensing: Current progress and future trends**](https://ieeexplore.ieee.org/document/10506064) <br>X. Li, C. Wen, Y. Hu, Z. Yuan, and X. X. Zhu.<br>| MGRS | 2024-04-22 | - | - |
| [**Language Integration in Remote Sensing: Tasks, datasets, and future directions**](https://ieeexplore.ieee.org/abstract/document/10278197) <br>L. Bashmal, Y. Bazi, F. Melgani, M. M. Al Rahhal, and M. A. Al Zuair.<br>| MGRS | 2023-10-11 | - | - |
| [**Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey**](https://ieeexplore.ieee.org/abstract/document/10278197) <br>L. Jiao et al.<br>| JSTARS | 2023-09-18 | - | - |



## Others
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**On the Foundations of Earth and Climate Foundation Models**](https://arxiv.org/pdf/2405.04285.pdf) <br>X. X. Zhu et al.<br>| arXiv | 2024-05-07 | [Github](https://github.com/zhu-xlab/EarthFoundationModels) | - |
| [**On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications**](https://arxiv.org/abs/2312.17016.pdf) <br>C. Tan et al.<br>| arXiv | 2023-12-23 | - | - |
| ![Star](https://img.shields.io/github/stars/jonathan-roberts1/charting-new-territories.svg?style=social&label=Star) <br> [**Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs**](https://arxiv.org/abs/2311.14656.pdf) <br>J. Roberts, T. L√ºddecke, R. Sheikh, K. Han, and S. Albanie. <br>| arXiv | 2023-11-24 | [Github](https://github.com/jonathan-roberts1/charting-new-territories) | - |
| [**The Potential of Visual ChatGPT for Remote Sensing**](https://www.mdpi.com/2072-4292/15/13/3232) <br>L. P. Osco, E. L. de Lemos, W. N. Gon√ßalves, A. P. M. Ramos, and J. Marcato Junior.<br>| Remote Sensing | 2023-06-22 | - | - |




# Awesome Datasets

## Datasets of Pre-Training for Alignment
|  Title  |   Venue  |   Date   |   Code   |   Note   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/SlytherinGe/RSTeller.svg?style=social&label=Star) <be> [**RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models**](https://arxiv.org/pdf/2408.14744.pdf) <br>J. Ge, Y. Zheng, K. Guo, and J. Liang.<br>| arXiv | 2024-08-27 | [Github](https://github.com/SlytherinGe/RSTeller) | [Link](https://huggingface.co/datasets/SlytherinGe/RSTeller) |
| ![Star](https://img.shields.io/github/stars/zhu-xlab/ChatEarthNet.svg?style=social&label=Star) <be> [**ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing**](https://arxiv.org/abs/2402.11325.pdf) <br>Z. Yuan, Z. Xiong, L. Mou, and X. X. Zhu.<br>| arXiv | 2024-02-17 | [Github](https://github.com/zhu-xlab/ChatEarthNet) | [Link](https://zenodo.org/records/11003436) |
| ![Star](https://img.shields.io/github/stars/om-ai-lab/RS5M.svg?style=social&label=Star) <br> [**RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing**](https://arxiv.org/abs/2306.11300.pdf) <br>Z. Zhang, T. Zhao, Y. Guo, and J. Yin.<br>| arXiv | 2024-01-02 | [Github](https://github.com/om-ai-lab/RS5M) | - |
| ![Star](https://img.shields.io/github/stars/wangzhecheng/SkyScript.svg?style=social&label=Star) <br> [**SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing**](https://doi.org/10.1609/aaai.v38i6.28393) <br>Z. Wang, R. Prabha, T. Huang, J. Wu, and R. Rajagopal.<br>| AAAI | 2024-03-24 | [Github](https://github.com/wangzhecheng/SkyScript) | [arXiv](http://arxiv.org/abs/2312.12856.pdf) |


<p align="center">
    <img src="./images/itpair.jpg" width="80%" height="100%">
</p>


## Datasets of Multimodal Instruction Tuning
| Name | Paper | Link | Note |
|:-----|:-----:|:----:|:-----:|
| **DDFAV** | [DDFAV: Remote Sensing Large Vision Language Models Dataset and Evaluation Benchmark](https://arxiv.org/pdf/2411.02733) | [Link](https://github.com/HaodongLi2024/rspope) | 27.7k | 
| **VRSBench** | [VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding](https://arxiv.org/pdf/2406.12384) | [Link](https://vrsbench.github.io/) | 29.6k | 
| **FIT-RS** | [SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding](http://arxiv.org/abs/2406.10100) | [Link](https://huggingface.co/datasets/ll-13/FIT-RS) | 1800.8k | 
| **RS-GPT4V** | [RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding](https://arxiv.org/abs/2406.09385) | [Link](https://github.com/GeoX-Lab/RS-GPT4V) | 991k | 
| **RS-instructions** | [RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery](https://www.mdpi.com/2072-4292/16/9/1477) | [Link](https://github.com/BigData-KSU/RS-LLaVA) | 7,058 | 
| **SkyEye-968k** | [SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model](https://arxiv.org/abs/2401.09712.pdf) | [Link](https://huggingface.co/datasets/ZhanYang-nwpu/SkyEye-968k) | 968k | 
| **Multi-task Instruction** | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544v2.pdf) | [Link](https://github.com/NJU-LHRS/LHRS-Bot) | 42,322 |
| **MMRS-1M** | [EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain](https://arxiv.org/abs/2401.16822.pdf) | [Link](https://github.com/wivizhang/EarthGPT) | >1M |
| **RS-ClsQaGrd-Instruct** | [H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model](https://arxiv.org/abs/2403.20213.pdf) | [Link](https://github.com/opendatalab/H2RSVLM) | 78k |
| **MMShip** | [Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery](https://arxiv.org/abs/2403.03790.pdf) | [Link]() | 81k |
| **RS-Specialized-Instruct** |[H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model](https://arxiv.org/abs/2403.20213.pdf) | [Link](https://github.com/opendatalab/H2RSVLM) | 29.8k |
| **RS multimodal instruction** | [GeoChat: Grounded Large Vision-Language Model for Remote Sensing](http://arxiv.org/abs/2311.15826.pdf) | [Link](https://github.com/mbzuai-oryx/geochat) | 318k |
| **LHRS-Instruct** | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544v2.pdf) | [Link](https://github.com/NJU-LHRS/LHRS-Bot) | 39.8k |
| **HqDC-Instruct** | [H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model](https://arxiv.org/abs/2403.20213.pdf) | [Link](https://github.com/opendatalab/H2RSVLM) | 30k |



<p align="center">
    <img src="./images/instruct.jpg" width="80%" height="100%">
</p>




# Latest Evaluation Benchmarks for Remote Sensing Vision-Language Tasks

## Remote Sensing Image Captioning and Aerial Video Captioning
<p align="center">
    <img src="./images/caption.jpg" width="80%" height="100%">
</p>


## Remote Sensing Visual Question Answering and Remote Sensing Visual Grounding
<p align="center">
    <img src="./images/vqavg.jpg" width="80%" height="100%">
</p>



## Remote Sensing Image-Text Retrieval
<p align="center">
    <img src="./images/itretrieval.jpg" width="80%" height="100%">
</p>

## Remote Sensing Scene Classification
<p align="center">
    <img src="./images/rssc.jpg" width="80%" height="100%">
</p>



## ü§ñ Contact
If you have any questions about this project, please feel free to contact zhanyangnwpu@gmail.com.



