# Awesome-Remote-Sensing-Multimodal-Large-Language-Model (Vision-Language)

üì¢ A collection of remote sensing multimodal large language model papers focusing on vision-language domains.



## Content
- [Papers](#papers)
- [related: Remote Sensing Vision-Language Foundation Models](#related)


## Papers
- üî• **Nov-30-23: Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs**

arXiv 2023 (arXiv:2311.14656). *.* [[Paper](https://arxiv.org/abs/2311.14656)][[Code](https://github.com/jonathan-roberts1/charting-new-territories)]

- üî• **Nov-28-23: GeoChat: Grounded Large Vision-Language Model for Remote Sensing**

arXiv 2023 (arXiv:2311.15826). *K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan.* [[Paper](http://arxiv.org/abs/2311.15826)][[Code](https://github.com/mbzuai-oryx/geochat)]

- üî• **Jul-28-23: RSGPT: A Remote Sensing Vision Language Model and Benchmark** 

arXiv 2023 (arXiv:2307.15266). *Y. Hu, J. Yuan, and C. Wen.* [[Paper](https://arxiv.org/abs/2307.15266)][[Code](https://github.com/Lavender105/RSGPT)]


## related: Remote Sensing Vision-Language Foundation Models
- üî• **Aug-10-23: RemoteCLIP: A Vision Language Foundation Model for Remote Sensing**

arXiv 2023 (arXiv:2306.11029). *F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, and J. Zhou.* [[Paper](https://arxiv.org/abs/2306.11029)][[Code](https://github.com/ChenDelong1999/RemoteCLIP)]


2 GRAFT
[1] U. Mall, C. P. Phoo, M. K. Liu, C. Vondrick, B. Hariharan, and K. Bala, ‚ÄúRemote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment.‚Äù arXiv, Dec. 11, 2023. Accessed: Jan. 15, 2024. [Online]. Available: http://arxiv.org/abs/2312.06960


