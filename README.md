# Awesome-Remote-Sensing-Multimodal-Large-Language-Model (Vision-Language)

üì¢ A collection of remote sensing multimodal large language model papers focusing on vision-language domains.



## Content
- [Papers](#papers)
- [related: Remote Sensing Vision-Language Foundation Models](#related)


## Papers
- üî• **Nov-30-23: Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs**

arXiv 2023 (arXiv:2311.14656). *.* [[Paper](https://arxiv.org/abs/2311.14656)][[Code](https://github.com/jonathan-roberts1/charting-new-territories)]

- üî• **Nov-28-23: GeoChat: Grounded Large Vision-Language Model for Remote Sensing**

arXiv 2023 (arXiv:2311.15826). *K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan.* [[Paper](http://arxiv.org/abs/2311.15826)][[Code](https://github.com/mbzuai-oryx/geochat)]

- üî• **Jul-28-23: RSGPT: A Remote Sensing Vision Language Model and Benchmark** 

arXiv 2023 (arXiv:2307.15266). *Y. Hu, J. Yuan, and C. Wen.* [[Paper](https://arxiv.org/abs/2307.15266)][[Code](https://github.com/Lavender105/RSGPT)]


## related: Remote Sensing Vision-Language Foundation Models
- üî• **Nov-30-23: Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs**

arXiv 2023 (arXiv:2311.14656). *F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, and J. Zhou.* [[Paper]()][[Code]()]

1 RemoteCLIP
[1] F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, and J. Zhou, ‚ÄúRemoteCLIP: A Vision Language Foundation Model for Remote Sensing.‚Äù arXiv, Jun. 19, 2023. Accessed: Jun. 26, 2023. [Online]. Available: http://arxiv.org/abs/2306.11029

2 GRAFT


